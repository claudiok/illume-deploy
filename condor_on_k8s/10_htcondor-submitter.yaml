#!/usr/bin/env kubectl apply -f
apiVersion: v1
kind: Service
# if available on the cluster: use a load balancer here
# (we just don't have those)
metadata:
  name: illume-sub
  namespace: illume
  labels:
    name: illume-sub
spec:
  ports:
    - name: "ssh"
      port: 22
  selector:
    app: illume-sub
---
# the service needs to be reachable on port 22
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ssh-to-illume-sub
  namespace: illume
spec:
  podSelector:
    matchLabels:
      app: illume-sub
  ingress:
  - ports:
    - protocol: TCP
      port: 22
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: illume-sub
  namespace: illume
spec:
  replicas: 1
  # we can handle parallel deployment
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: illume-sub
  serviceName: "illume-sub"
  template:
    metadata:
      labels:
        app: illume-sub
      # we need this to make Singularity work: (more privileges)
      annotations:
        container.apparmor.security.beta.kubernetes.io/sub: unconfined
    spec:
      initContainers:
      - name: create-scratch-loopback
        # create the /scratch/scratch_loop directory before starting anything else
        image: ubuntu:16.04
        command: ["/bin/sh"]
        # TODO: get the size (count) from the pod spec via env
        args: ["-c", "/usr/bin/fallocate -l 100G /scratch/scratch.img && mkfs.ext4 /scratch/scratch.img && mkdir -p /scratch/scratch_loop"]
        volumeMounts:
          - name: scratch
            mountPath: /scratch
      containers:
      - name: scratch-loopback
        image: ubuntu:16.04
        command: ["/bin/sh"]
        args: ["-c", "mount -o loop /scratch/scratch.img /scratch/scratch_loop && /bin/sleep infinity"]
        lifecycle:
          preStop:
            exec:
              # Do a lazy unmount of the loopback /scratch we created in the initContainer.
              command: ["/bin/umount","-l","/scratch"]
        securityContext:
          privileged: true
        volumeMounts:
          - name: scratch
            mountPath: /scratch
            mountPropagation: Bidirectional
      - name: sub
        image: illumecluster/htcondor-schedd:20200207
        resources:
          requests:
            memory: "8.0Gi"
            cpu: "8000m"
            # ephemeral-storage: "30Gi" # this is everything except for scratch
          limits:
            memory: "8.0Gi"
            cpu: "8000m"
            # ephemeral-storage: "30Gi" # this is everything except for scratch
        # we need this to make Singularity work: (more privileges)
        securityContext:
          capabilities:
            add:
              - SYS_ADMIN
        env:
        - name: CONDOR_HOST
          value: "illume-coll"
        - name: SEC_PASSWORD_FILE
          value: "/etc/condor/pool_password/password"
        - name: LDAP_SERVER
          value: "openldap.illume"
        - name: LDAP_BASEDN
          value: "dc=illume,dc=systems"
        volumeMounts:
          - name: pool-password
            mountPath: /etc/condor/pool_password
          - name: ssh-host-keys
            mountPath: /etc/ssh-host-keys
          - name: homedir
            mountPath: /home
            subPath: illume/home
          - name: cvmfs
            mountPath: /cvmfs
          - name: scratch
            mountPath: /scratch
            subPath: scratch_loop
          - name: datadir
            mountPath: /data
        ports:
        - containerPort: 9618
          protocol: TCP
        - containerPort: 22
          protocol: TCP
        livenessProbe:
          exec:
            command:
            - /usr/local/bin/htcondor-schedd-liveness
          initialDelaySeconds: 10
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 5
      volumes:
      - name: pool-password
        secret:
          defaultMode: 0600
          secretName: htcondor-pool-password
      - name: ssh-host-keys
        secret:
          defaultMode: 0600
          secretName: ssh-host-keys
      - name: homedir
        nfs:
          server: 192.168.19.2
          path: "/mnt/tank/export/scratch/icecube"
      - name: datadir
        nfs:
          server: 192.168.19.6
          path: "/mnt/tank/export/scratch/icecube"
      - name: cvmfs
        hostPath:
          path: "/cvmfs"
      - name: scratch
        emptyDir: {}
        #  sizeLimit: 100Gi
