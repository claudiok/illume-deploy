# perform these steps after ansible is done:

# need a more modern version of rke than v0.1.1 since
# we have custom ssh ports on our nodes.
git clone https://github.com/rancher/rke
cd rke
make
sudo cp bin/rke /usr/local/bin/rke
cd ~/

# bring up the kubernetes cluster
rke up

# activate the configuration
cd ~
mkdir .kube
cp kube_config_cluster.yml .kube/config

# taint control nodes so that pods don't end up on them
kubectl taint nodes illume-control-01 illume-control-02 illume-control-03 node-role.kubernetes.io/master=true:NoSchedule

# install the nvidia device plugin
kubectl create -f nvidia-device-plugin.yml

# taint all GPU nodes
kubectl taint nodes illume-worker-1080ti-01 illume-worker-1080ti-02  illume-worker-1080ti-03nvidia.com/gpu=true:NoSchedule


# define some network policies:

# label the kube-system namespace so we can refer to it in our
# NetworkPolicy definitions
kubectl label namespace kube-system name=kube-system




# create and label our namespace
kubectl create -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: illume
  labels:
    name: illume
EOF

# allow intra-namespace communication (and prevent all other communication)
kubectl create -f - <<EOF
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-namespace-illume
  namespace: illume
spec:
  podSelector: {}
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: illume
  egress:
    - to:
      - namespaceSelector:
          matchLabels:
            name: illume
EOF

# allow DNS access from illume namespace
kubectl create -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: illume
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
EOF

# allow public internet access from the illume namespace
kubectl create -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-public-internet
  namespace: illume
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
          cidr: 0.0.0.0/0
          except:
            - 10.0.0.0/8     # RFC1918 block 1
            - 172.16.0.0/12  # RFC1918 block 2
            - 192.168.0.0/16 # RFC1918 block 3
            - 169.254.0.0/16 # link-local
            - 224.0.0.0/24   # multicast
EOF




---

### deploy rook

Now set up some ceph storage on top of the bulk ephemeral storage we have.
We set this up for block storage we will provide to nodes for scratch space.

```
git clone https://github.com/rook/rook
cd rook/cluster/examples/kubernetes
```

Edit rook-operator.yaml and make env FLEXVOLUME_DIR_PATH point to /var/lib/kubelet/volumeplugins.
Also change AGENT_TOLERATION to "NoSchedule".

```
kubectl create -f rook-operator.yaml
```

Now wait until
```
kubectl -n rook-system get pod
```
shows that everything is created.


Label the storage nodes
```
kubectl label nodes illume-worker-1080ti-01 role=storage-node
kubectl label nodes illume-worker-1080ti-02 role=storage-node
kubectl label nodes illume-worker-1080ti-03 role=storage-node
kubectl label nodes illume-worker-nogpu-01 role=storage-node
kubectl label nodes illume-worker-nogpu-02 role=storage-node
kubectl label nodes illume-worker-nogpu-03 role=storage-node
```
and deploy. Note that this can tolerate GPU nodes and will only run
on labeled nodes.
```
kubectl create -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: rook
---
apiVersion: rook.io/v1alpha1
kind: Cluster
metadata:
  name: rook
  namespace: rook
spec:
  dataDirHostPath: /cephstore
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter:
    metadataDevice:
    location:
    storeConfig:
      storeType: bluestore
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
    api:
      nodeAffinity:
      tolerations:
    mgr:
      nodeAffinity:
      tolerations:
    mon:
      nodeAffinity:
      tolerations:
    osd:
      nodeAffinity:
      tolerations:
EOF
```

Monitor until we see all pods (mgr, osd, mon, api)
```
kubectl -n rook get pods
```

Create a rook storage pool and storage class for block storage:
```
kubectl create -f - <<EOF
apiVersion: rook.io/v1alpha1
kind: Pool
metadata:
  name: replicapool
  namespace: rook
spec:
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: rook-block
   namespace: rook
provisioner: rook.io/block
parameters:
  pool: replicapool
EOF
```

See how things worked:
```
kubectl create -f rook-tools.yaml
kubectl -n rook get pod rook-tools
kubectl -n rook exec -it rook-tools bash
rookctl status
ceph df
rados df
kubectl delete -f rook-tools.yaml
```

#Now create a rook filesystem:
#```
#kubectl create -f - <<EOF
#apiVersion: rook.io/v1alpha1
#kind: Filesystem
#metadata:
#  name: myfs
#  namespace: rook
#spec:
#  metadataPool:
#    replicated:
#      size: 3
#  dataPools:
#    - erasureCoded:
#       dataChunks: 2
#       codingChunks: 1
#  metadataServer:
#    activeCount: 1
#    activeStandby: true
#EOF
#
#kubectl -n rook get pod -l app=rook-ceph-mds
#```
#
#And test it:
#```
#kubectl create -f rook-tools.yaml
#kubectl -n rook get pod rook-tools
#kubectl -n rook exec -it rook-tools bash
#rookctl status
#ceph df
#rados df
#
#mkdir /tmp/registry
#rookctl filesystem mount --name myfs --path /tmp/registry
#
#rookctl filesystem unmount --path /tmp/registry
#rmdir /tmp/registry
#```


-----------------

# expose service default/sshd-jumpserver-svc on TCP port 22 through ingress server
kubectl replace -f - <<EOF
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
data:
  22: "default/sshd-jumpserver-svc:22"
EOF
