# perform these steps after ansible is done:

# need a more modern version of rke than v0.1.1 since
# we have custom ssh ports on our nodes.
git clone https://github.com/rancher/rke
cd rke
make
sudo cp bin/rke /usr/local/bin/rke
cd ~/

# bring up the kubernetes cluster
rke up

# activate the configuration
cd ~
mkdir .kube
cp kube_config_cluster.yml .kube/config

# install
kubectl create -f nvidia-device-plugin.yml


# taint all GPU nodes
kubectl taint nodes illume-worker-1080ti-01 nvidia.com/gpu=true:NoSchedule
kubectl taint nodes illume-worker-1080ti-02 nvidia.com/gpu=true:NoSchedule
kubectl taint nodes illume-worker-1080ti-03 nvidia.com/gpu=true:NoSchedule


# network policies:

# label the kube-system namespace so we can refer to it in our
# NetworkPolicy definitions
kubectl label namespace kube-system name=kube-system


# create and label our namespace
kubectl create -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: freeforall
  labels:
    name: freeforall
EOF


# allow intra-namespace communication (and prevent all other communication)
kubectl create -f - <<EOF
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: networkpolicy-freeforall
  namespace: freeforall
spec:
  podSelector: {}
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: freeforall
  egress:
    - to:
      - namespaceSelector:
          matchLabels:
            name: freeforall
EOF


# allow DNS access from freeforall namespace
kubectl create -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
  namespace: freeforall
spec:
  podSelector:
    matchLabels: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53
EOF


# allow public internet access from freeforall namespace
kubectl create -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-public-internet
  namespace: freeforall
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
          cidr: 0.0.0.0/0
          except:
            - 10.0.0.0/8     # RFC1918 block 1
            - 172.16.0.0/12  # RFC1918 block 2
            - 192.168.0.0/16 # RFC1918 block 3
            - 169.254.0.0/16 # link-local
            - 224.0.0.0/24   # multicast
EOF


# expose service default/sshd-jumpserver-svc on TCP port 22 through ingress server
kubectl replace -f - <<EOF
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
data:
  22: "default/sshd-jumpserver-svc:22"
EOF


---

### deploy rook
```
git clone https://github.com/rook/rook
cd rook/cluster/examples/kubernetes
```

Edit rook-operator.yml and make env FLEXVOLUME_DIR_PATH point to /var/lib/kubelet/volumeplugins.
Also change AGENT_TOLERATION to "NoSchedule" (and optionally AGENT_TOLERATION_KEY to "nvidia.com/gpu").

```
kubectl create -f rook-operator.yaml
```

wait until
```
kubectl -n rook-system get pod
```
shows that everything is created.

Now label the storage nodes
```
kubectl label nodes illume-worker-1080ti-01 role=storage-node
kubectl label nodes illume-worker-1080ti-02 role=storage-node
kubectl label nodes illume-worker-1080ti-03 role=storage-node
kubectl label nodes illume-worker-nogpu-01 role=storage-node
kubectl label nodes illume-worker-nogpu-02 role=storage-node
kubectl label nodes illume-worker-nogpu-03 role=storage-node
```

And deploy. Note that this can tolerate GPU nodes and will only run
on labeled nodes.
```
apiVersion: v1
kind: Namespace
metadata:
  name: rook
---
apiVersion: rook.io/v1alpha1
kind: Cluster
metadata:
  name: rook
  namespace: rook
spec:
  dataDirHostPath: /mnt
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter:
    metadataDevice:
    location:
    storeConfig:
      storeType: bluestore
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
    api:
      nodeAffinity:
      tolerations:
    mgr:
      nodeAffinity:
      tolerations:
    mon:
      nodeAffinity:
      tolerations:
    osd:
      nodeAffinity:
      tolerations:
```

Now create a rook filesystem:
```
kubectl create -f - <<EOF
apiVersion: rook.io/v1alpha1
kind: Filesystem
metadata:
  name: myfs
  namespace: rook
spec:
  metadataPool:
    replicated:
      size: 3
  dataPools:
    - erasureCoded:
       dataChunks: 2
       codingChunks: 1
  metadataServer:
    activeCount: 1
    activeStandby: true
EOF

kubectl -n rook get pod -l app=rook-ceph-mds
```

And test it:
```
kubectl create -f rook-tools.yaml
kubectl -n rook get pod rook-tools
kubectl -n rook exec -it rook-tools bash
rookctl status
ceph df
rados df

mkdir /tmp/registry
rookctl filesystem mount --name myfs --path /tmp/registry

rookctl filesystem unmount --path /tmp/registry
rmdir /tmp/registry
```
